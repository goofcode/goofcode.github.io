---
title: 딥러닝 기초 - 오차역전파(back propagation) 알고리즘
tags: [블로그,CS]
---
 

## 1.역전파(back propagation) 알고리즘은 무엇일까?
역전파 알고리즘이라는 것은 인공지능의 한 분야인 인공신경망, artificial neural network, ANN를 학습시키기 위한 가장 기본적이고 일반적인 알고리즘이라고 할 수 있다(계산적 편의성이 크다). 역전파라는 이름은 오차(에러)가 본래 진행방향과 반대방향으로 전파 된다하여 붙여진 이름이다.(backward propagation)

## 2. 그럼 ANN, MLP이란?
사실 인공 신경망(ANN)이라는 것은 아주 오래 전부터 연구의 대상이었는데, 그 기초는 퍼셉트론(perceptron)이라고 하는 신경망이다. 퍼셉트론은 기초 수학에서 배우는 방정식, 선형대수학을 배웠다면 linear combination과 비슷한 형태이다. 간단하게 그림으로 나타내면 다음과 같다.

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/%E1%84%80%E1%85%B3%E1%84%85%E1%85%B5%E1%86%B71.jpg)

즉, input 1, 2, 3에 대해서 output은 다음과 같이 계산된다.

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/2418053F58669F810D.png)

여기서 f를 활성함수(activation function)이라 하는데, 결과의 범위를 제한하고 계산의 편의성을 제공한다.

만약 input과 output 값을 모두 알고 있다면, w, weight를 찾아내는 것이 가능하다. 이를 통해 weight가 학습되면, 해당 퍼셉트론은 input에서 output을 산출할 수 있게된다. 

단순하고 강력한 퍼셉트론은 초반에는 굉장한 이목을 끌었지만, 이내 한계가 드러나게 된다. 일차 방정식은 (기하학적으로) 선에 불과하기때문에, 학습할 수 있는 범위가 매우 제한적이다. 

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/xor.jpg)

대표적인 논리연산들을 살펴보자. 차례로 AND, OR, XOR이다. 쉬운 이해를 위해 색이 칠해진 점을 1, 칠해지지 않은 점을 0이라 생각해보자. AND나 OR같은 경우에는 1과 0의 결과를 선으로 분리할 수 있지만, XOR의 경우에는 하나의 직선으로 분리가 불가능하다. 즉, 단층의 퍼셉트론으로 해결 불가능하다는 결론에 이를 수 있다. 

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/99F24A495B7F84D714.jpg)

이를 해결하기 위한 방법으로 제시된 것이 층, layer로 쌓은 퍼셉트론이다. 이를 다층 신경망, multi-layer perceptron, MLP라고 한다. 현존하는 딥러닝 기술 중 neural 신경이란 이름을 가진 것들은 대부분 다층 신경망을 사용한다고 보면 된다. 이를 통해 퍼셉트론에서 제시된 문제들을 충분히 해결 가능하다.

## 3. 역전파 알고리즘의 특징은?
역전파 알고리즘은 input과 output을 알고 있는 상태에서(이를 supervised learning이라 함) 신경망을 학습시키는 방법이다. 역전파 알고리즘을 적용시키기 이전에 MLP에 대해 몇가지 알아야할 것들이 있다.

1. 초기 가중치, weight값은 랜덤으로 주어진다.
2. 각각 노드, node는 하나의 퍼셉트론으로 생각한다. 즉, 노드를 지나칠 때마다 활성함수를 적용한다. 활성함수를 적용하기 이전을 net, 이후를 out이라고 하겠다. 다음 레이어의 계산에는 out값을 사용한다. 마지막 out이 output이 된다.
3. 활성함수는 [시그모이드 sigmoid 함수](https://en.wikipedia.org/wiki/Sigmoid_function)로 한다. (미분하기 용이해서 대표적으로 쓰이는 함수이지만, 다른 활성함수가 아닌 다른 활성함수를 사용해도 문제 없다.)

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/99F24A495B7F84D714%202.jpg)

우리가 결과값으로 얻기를 바라는 값을 target, 실제로 얻은 결과값을 output이라하면, 오차 E는 다음과 같다.

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/273A59455870564421.png)

여기서 합(sum)의 의미는 모든 output(output1, output2 …)에서 발생한 오차를 모두 더해주는 것이다. 최종목적은 이 오차에 관한 함수 E의 함수값을 0에 근사시키는 것이다. 오차가 0에 가까워진다면, 신경망은 학습에 사용된 input들과 그에 유사한 input에 대해서 우리가 원하는 output, 정답이라고 할 수 있는 값들을 산출할 것이다. 	

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/2319F94C587240B22B.png)

각각 가중치가 2개일때 오차 E를 도식화. 그래프 상에서 y=0, w1, w2 =0일때 E가 최소가 됨. (위키백과)

오차 E를 모든 가중치 w1 ... wn에 대한 방정식으로 본다면, 우리가 해야하는 것은 가중치 w를 수정해 E가 최소가 되도록 만드는 것이다. 이를 위해 사용하는 것이 경사 감소법, gradient descent라는 최적화 알고리즘이다. 기본원리는 기울기가 낮은 쪽으로 연속적으로 이속시켜 값이 최소가 되는 점(극값)에 다다르게 하는 것이다. 

이를 위해 오차 E를 미분하는 과정이 필요한데, 모든 가중치가 오차 E에 영향을 미치고 있으므로 E를 각각의 가중치로 편미분한다. 다층 신경망은 각 계층이 연결되어 있어, output에 가까운 미분 과정에서 사용되는 값이 output에서 먼쪽의 미분과정에 사용된다. 따라서 output과 가까운 쪽의 미분을 먼저 진행한다.

## 4. 알고리즘

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/2674D73C586F43C229.jpg)

알고리즘 자체는 굉장히 단순하다. 크게 4단계로 이루어져있는데, 사실상 1회(1 epoch, 세대)는 3번까지의 단계이다. 한번의 실행으로는 오차가 많이 줄어들지는 않으나, 여러번 반복해서 실행하면 오차가 0에 가까워진다.

1. 기존에 설정되어있는 가중치를 사용해 net, out을 계산한다. (forward pass)
2. 전체 오차를 각 가중치로 편미분한 값을 기존의 가중치에서 빼준다.(오차를 줄인다. 편미분값을 더하면 최대값을 찾는 과정)
3. 모든 가중치에 대해 2를 실행한다.(output에 가까운 쪽에서부터 먼쪽으로) 
4. 1 ~3을 학습 회수만큼 반복한다.

위에서도 언급했듯, output에 가까운쪽에서부터 갱신하는 이유는 가까운 쪽에서 사용한 값을 먼쪽의 계산 과정에서 다시 사용하기 때문이다.

- - - -

그러면 자세히 알아보자.

#### 1. 단순히 input에 가중치를 곱한 값들을 더하고 활성함수를 이용해 out을 만드는 과정을 반복한다.

#### 2-1. w5를 먼저 기준으로 잡아보자. 이때 전체 오차 E를 w5로 편미분한 값을 기존의 w5에서 빼주면 w5의 1회 갱신이 완료된 것이다. 

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/272B613F5871057D1E.png)

오차 E를 w5로 편미분하면, chain rule을 적용해 다음과 같이 풀어낼 수 있고, 각각을 계산하는 방법은 다음과 같다. 

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/233D6734587223D122.png)

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/226E514E58705A131A.png)

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/227D4B45587065E122.png)

out o1은 다른 오차에 영향을 미치지 않으므로, 오차 E1을 out o1으로 미분한 값이 (1)과 같아진다. 

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/2639453B58705B4702.png)

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/27020B3D587066652B.png)

out과 net의 관계는 활성함수이므로 활성함수(이 경우에 시그모이드함수)를 미분한 것과 같은 값이 된다.

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/211B1238587064CF2F.png)

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/2155AD43587066D530.png)

net은 각각의 가중치와 이전 노드의 곱의 합으로 이루어져있으므로, (3)은 이전 노드의 out값이 된다.

특히, (1), (2)를 곱한 값은 노드 o1의 delta 값이라 하고 이후 과정에 사용된다. 따라서 알고리즘의 속도 향상을 위해 실제 코드에서는 이를 저장해 놓는 과정이 필요하다.

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/2344D4495872294616.png)

따라서 w5는 다음과 같이 갱신된다.

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/246279415871039F1B.png)

이는 hidden 레이어와 output 레이어 사이에 존재하는 가중치 값들에 대해 모두 성립한다. 따라서, 

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/2674D73C586F43C229%202.jpg)

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/256B9645587103D930.png)
![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/257E193E587103F728.png)
![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/27400D43587104183A.png)


#### 2-2.  다음으로 w1을 기준으로 잡으면, w5와 마찬가지로, 다음과 같은 형태로 가중치를 갱신할 수 있다.

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/231CF545587105C008.png)

다만, w1은 w5에 비해 output에서 멀리 떨어져 있기때문에, 편미분의 결과값이 조금 다르다. w1과 마찬가지로 다음과 같이 풀어쓰면, 

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/215C3C345872245806.png)

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/24110E3B5946EAC904.png)

out h1은 output 1과 output 2의 오차 모두에 영향을 준다. 따라서 각각의 오차를 편미분한 값의 합으로 나타낼 수 있고, 이는 각 노드의 델타 값과 그에 대응하는 가중치값의 곱들의 합으로 나타내어진다.

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/245C054A587227D531.png)

w5의 (2)에서 했던 것과 동일한 시그모이드 함수의 미분.

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/2166EE48587228292A.png)

마찬가지로 이전의 (3)과 동일한 원리. 

여기서 노드 h1의 델타값 역시 (1)과 (2)의 곱이다.

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/2146FF415872297618.png)

따라서 w1은 다음과 같이 갱신된다.

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/22779B4058722A7D2E.png)

다른 가중치 역시 마찬가지이다.

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/2674D73C586F43C229%203.jpg)

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/277F024D58722B513E.png)
![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/2227294E58722B6F2C.png)
![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/22328B4F58722B8B22.png)

이렇게 레이어 3개 사이에 생긴 가중치를 갱신하면서 알게 된 점은 가중치를 갱신하는 식이 다음과 같다는 것이다.

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/237D7D4A5872395101.png)

이때 델타값을 구하는 식은 노드가 output 레이어의 속한 경우와 속하지 않은 경우로 나눌 수 있다

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/2457044A5872399B2A.png)

![](/assets/img/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20-%20%E1%84%8B%E1%85%A9%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%91%E1%85%A1(back%20propagation)%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7/2672934D5872398A26.png)

좀 더 이해하기 쉬운 의사코드로 나타내면
```c
for training_repeat
  for training_sets

		// 가중치로 net, out 계산 
		forward pass              

		//output 레이어에서 시작해 input + 1 레이어로
		for i = output layer -> i = input layer + 1     
			for all nodes in layer i
				set delta
			
			//layer i와 i-1 사이에 있는 가중치 갱신
		  	for all weights between layer i and layer i-1  
				update weight                           
```

## 5. 역전파 알고리즘의 한계
역전파 알고리즘은 가장 보편적으로 쓰이는 알고리즘임에도 한계를 지니고 있다. 경사 감소법의 한계에서 오는 것인데, 경사 감소법 알고리즘은 항상 전역 최소값 global minimum을 찾는다고 보장할 수 없다. 극소값이 두개 이상 존재하는 함수에 대해 가장 작은 최소값을 찾는다고 할 수 없다. 알고리즘이 단순히 기울기가 작아지는 방향으로 움직이는 것이기 때문에, 시작점에 따라 결과가 달라질수 있다.

[참고한 자료](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/) 